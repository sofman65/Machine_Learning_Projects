{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author : Sofianos Lampropoulos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook I have built an end-to-end sentiment classification system from scratch. The system accepts a movie review as input and classifies it as either positive or negative. There are three main steps:\n",
    "\n",
    "* **Preprocess** - Split the data into train and test sets, tokenize, stem words, create bag-of-words features, etc.\n",
    "* **Models** - Create and experiment with different models: GaussianNB, Gradient-Boosted Decision Tree classifier and Recurrent Neural Network.\n",
    "* **Evaluation** - Compare the performances of the models and summarize steps to make the chosen model do better.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "[Step 1: Explore the data](#step1)\n",
    "\n",
    "[Step 2: Preprocess the data](#step2)\n",
    "\n",
    "[Step 3: Extract Bag-of-words features](#step3)\n",
    "\n",
    "[Step 4: Classification using BoW features](#step4)\n",
    "\n",
    "[Step 5: Switching gears - RNNs](#step5)\n",
    "\n",
    "[Step 6: Evaluation](#step6)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exploring the data <a class=\"anchor\" id=\"step1\"></a>\n",
    "\n",
    "The [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/) consists of movie reviews from the website [imdb.com](http://www.imdb.com/), each labeled as either '**pos**itive', if the reviewer enjoyed the film, or '**neg**ative' otherwise.\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011.\n",
    "\n",
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews: train = 0 pos / 0 neg, test = 0 pos / 0 neg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "def read_imdb_data(data_dir='data/imdb-reviews'):\n",
    "    \"\"\"Read IMDb movie reviews from given directory.\n",
    "    \n",
    "    Directory structure expected:\n",
    "    - data/\n",
    "        - train/\n",
    "            - pos/\n",
    "            - neg/\n",
    "        - test/\n",
    "            - pos/\n",
    "            - neg/\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Data, labels to be returned in nested dicts matching the dir. structure\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "    # Assume 2 sub-directories: train, test\n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "\n",
    "        # Assume 2 sub-directories for sentiment (label): pos, neg\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            # Fetch list of files for this sentiment\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            # Read reviews data and assign labels\n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    labels[data_type][sentiment].append(sentiment)\n",
    "            \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "    \n",
    "    # Return data, labels as nested dicts\n",
    "    return data, labels\n",
    "\n",
    "# Read imdb data and write them into pickle files\n",
    "if not (os.path.isfile('data.pickle') and os.path.isfile('labels.pickle')):\n",
    "    data, labels = read_imdb_data()\n",
    "    with open(\"data.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(\"labels.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "# Open the pickle files\n",
    "if os.path.getsize(\"data.pickle\") > 0:\n",
    "    with open(\"data.pickle\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "else:\n",
    "    print (\"You're trying to load an empty file\")\n",
    "\n",
    "if os.path.getsize(\"data.pickle\") > 0:\n",
    "    with open(\"labels.pickle\", \"rb\") as handle:\n",
    "        labels = pickle.load(handle)\n",
    "else:\n",
    "    print (\"You're trying to load an empty file\"        )\n",
    "\n",
    "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "        len(data['train']['pos']), len(data['train']['neg']),\n",
    "        len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded in, let's take a quick look at one of the positive reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hood of the Living Dead had a lot to live up to even before the opening credits began. First, any play on \"...of the living dead\" invokes His Holiness Mr. Romero and instantly sets up a high standard to which many movies cannot afford to aspire. And second, my movie-watching companion professed doubt that any urban horror film would surpass the seminal Leprechaun In the Hood. Skeptical, we settled in to watch. <br /><br />We were rewarded with a surprisingly sincere and good-hearted zombie film. Oh, certainly the budget is low, and of course the directors' amateurs friends populate the cast, but Hood of the Living Dead loves zombie cinema. Cheap? Yeah. But when it's this cheap, you can clearly see where LOVE holds it together. <br /><br />Ricky works in a lab during the day and as a surrogate parent to his younger brother at night. He dreams of moving out of Oakland. Before this planned escape, however, his brother is shot to death in a drive-by. Ricky's keen scientific mind presents an option superior to CPR or 911: injections of his lab's experimental regenerative formula. Sadly, little bro wakes up in an ambulance as a bloodthirsty Oakland zombie! Chaos and mayhem! I think it's more economical to eat your enemies than take vengeance in a drive-by, but then again, I'm a poor judge of the complexities of urban life. (How poor a judge? In response to a gory scene involving four men, I opined \"Ah-ha! White t-shirts on everyone so the blood shows up. Economical! I used the same technique in my own low-budget horror film.\" Jordan replied, \"No, that's gang dress. White t-shirts were banned from New Orleans bars for a time as a result.\" Oh.)<br /><br />A lot of the movie is set in someone's living room, so there's a great deal of hanging out and waiting for the zombies. But the characters are sympathetic and the movie is sincere-- it surpasses its budget in spirit. <br /><br />Zombie explanation: When man plays God, zombies arise! Or, perhaps: Follow FDA-approved testing rules before human experimentation! <br /><br />Contribution to the zombie canon: This is the first zombie movie I've seen with a drive-by shooting. As far as the actual zombies go, infection is spread with a bite as usual, but quite unusually head shots don't work-- it's heart shots that kill. Zombies have pulses, the absence of which proves true death. And these zombies make pretty cool jaguar-growl noises. <br /><br />Gratuitous zombie movie in-joke: A mercenary named Romero. Groan. <br /><br />Favorite zombie: Jaguar-noise little brother zombie, of course!\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['pos'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one with a negative sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was nothing about this movie that I liked. It was so obviously low-budget with bad lighting and camera work (almost like Blair Witch Project, only it wasn't supposed to be that way). There wasn't really much to the plot, and the movie just drug on and on. I actually fast-forwarded through the last 1/3 of the movies, but that did not help matters much. It looked like it might be good from the box, but I must say again: nothing about this movie even resembled good. No good actors, the special effects were so fake, the camera work was horrible, and the dialogue was painfully terrible. On my own personal scale, I give this movie a 0 of 10. Yikes!\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['neg'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a wordcloud visualization of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7e30e7b85012>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwordcloud_by_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def wordcloud_by_sentiment(sentiment):\n",
    "    \"\"\"Create a wordcloud by sentiment.\n",
    "    \n",
    "    Parameters:\n",
    "    sentiment: \"pos\"/\"neg\" string\n",
    "    \n",
    "    Returns: \n",
    "    A wordcloud of the reviews that have the given sentiment    \n",
    "    \"\"\"    \n",
    "    # Combine all reviews for the desired sentiment\n",
    "    combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
    "\n",
    "    # Initialize wordcloud object\n",
    "    wc = WordCloud(background_color='white', max_words=50,\n",
    "            # update stopwords to include common words like film and movie\n",
    "            stopwords = STOPWORDS.update(['br','film','movie']))\n",
    "\n",
    "    # Generate and plot wordcloud\n",
    "    plt.imshow(wc.generate(combined_text))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create a wordcloud from positive reviews    \n",
    "wordcloud_by_sentiment('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordcloud_by_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3a1b2b951c56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create a wordcloud from negative reviews\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwordcloud_by_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'neg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'wordcloud_by_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a wordcloud from negative reviews\n",
    "wordcloud_by_sentiment('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form training and test sets\n",
    "\n",
    "Combine the positive and negative documents to get one unified training set and one unified test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    # Combine positive and negative reviews and labels\n",
    "    data_train = data[\"train\"][\"pos\"] + data[\"train\"][\"neg\"]\n",
    "    data_test = data[\"test\"][\"pos\"] + data[\"test\"][\"neg\"]\n",
    "    labels_train = [\"pos\"] * len(data[\"train\"][\"pos\"] ) + [\"neg\"] * len(data[\"train\"][\"neg\"])\n",
    "    labels_test = [\"pos\"] * len(data[\"test\"][\"pos\"] ) + [\"neg\"] * len(data[\"test\"][\"neg\"])\n",
    "    \n",
    "    # Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train,labels_train)\n",
    "    data_test, labels_test = shuffle(data_test,labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = prepare_imdb_data(data)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(data_train), len(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preprocessing <a class=\"anchor\" id=\"step2\"></a>\n",
    "\n",
    "As seen in the sample reviews, the raw data includes HTML. Therefore there are HTML tags that need to be removed. We also need to remove non-letter characters, normalize uppercase letters by converting them to lowercase, tokenize, remove stop words, and stem the remaining words in each document.\n",
    "\n",
    "### Convert each review to words\n",
    "\n",
    "The function `review_to_words()` performs all these steps. First, let's import all the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sofma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# BeautifulSoup to easily remove HTML tags\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# RegEx for removing non-letter characters\n",
    "import re\n",
    "\n",
    "# NLTK library for the remaining steps\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")   # download list of stopwords\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'test', 'would', 'make', 'great', 'movi', 'review']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\"Convert a raw review string into a sequence of words.\"\"\"\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    clean_text = BeautifulSoup(review, \"html5lib\").get_text()\n",
    "    \n",
    "    # Remove non-letters using RegEx\n",
    "    clean_text = re.sub(r\"[^a-zA-Z]\", \" \", clean_text)\n",
    "    \n",
    "    # Convert to lowercase and split text into words\n",
    "    words = (clean_text.lower()).split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Reduce words to their stems\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    # Return final list of words\n",
    "    return words\n",
    "\n",
    "review_to_words(\"\"\"This is just a <em>test</em>.<br/><br />\n",
    "But if it wasn't a test, it would make for a <b>Great</b> movie review!\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function `review_to_words()` fully implemeneted, we can apply it to all reviews in both training and test datasets. This may take a while, so let's build in a mechanism to write to a cache file and retrieve from it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n",
      "\n",
      "--- Raw review ---\n",
      "What a pleasant surprise: A Disney DTV (Direct to Video) sequel that's actually GOOD. \"The Lion King 1 1/2\" is a comedy affair that involves everyone's favorite meerkat and warthog, Timon and Pumbaa. It starts out with them watching the original movie and making comments, until Timon complains that \"we're not here yet\". After fighting over the remote, Timon and Pumbaa decide to tell the audience \"their story\". They start the new movie, and the laughs begin.<br /><br />To be honest, I was losing hope in Disney. Most of their direct-to-video fare has been aggressively awful, with the story lines desperate to cash in on the original. \"1 1/2\" decides to try something different: It tells the original story from a new point of view. Okay, it adds some stuff, like where Timon came from and how he and Pumbaa met, but it's an interesting concept. Nathan Lane and Ernie Sabella perfectly recapture the bond and friendship that made us love them in the first movie. There's also some neat movie parodies (including one scene that simultaneously spoofs 'The Good, the Bad, and the Ugly' and 'Cool Hand Luke), and the script is filled with funny memorable lines.<br /><br />Added to the cast are Julie Kavner (\"The Simpsons\") as Timon's worry-wart mom, and Jerry Stiller as Timon's slightly crazy Uncle Max. Matthew Broderick shows up as Simba, and his \"teen\" scene reminded me of Ferris Bueller. Everyone does well, especially Robert Guillame (is that how you spell it?) as Rafiki, who exudes more and more Yoda-speak in this movie (Timon comments on this several times).<br /><br />'The Lion King 1 1/2' is a perfect alternate choice for those of you getting tired of the Shrekification of animation. 10/10\n",
      "\n",
      "--- Preprocessed words ---\n",
      "['bore', 'unhappi', 'young', 'babe', 'zandale', 'winningli', 'sultri', 'vibrant', 'perform', 'lusciou', 'brunett', 'knockout', 'erika', 'anderson', 'feel', 'trap', 'stale', 'loveless', 'marriag', 'fail', 'poet', 'decent', 'yet', 'dull', 'businessman', 'thierri', 'martin', 'solid', 'credibl', 'portray', 'judg', 'reinhold', 'zandale', 'torrid', 'adulter', 'fling', 'sleazi', 'arrog', 'artist', 'johnni', 'collin', 'delici', 'play', 'slimi', 'hilt', 'nicola', 'cage', 'relationship', 'thierri', 'zandale', 'salvag', 'everyth', 'go', 'fall', 'apart', 'go', 'seed', 'director', 'sam', 'pillsburi', 'screenwrit', 'mari', 'kornhaus', 'lay', 'tawdri', 'soap', 'opera', 'style', 'histrion', 'someth', 'thick', 'attempt', 'tell', 'wannab', 'seriou', 'insight', 'stori', 'desir', 'run', 'amok', 'potenti', 'danger', 'consequ', 'plot', 'goe', 'glorious', 'rail', 'laughabl', 'histrion', 'last', 'third', 'dialogu', 'likewis', 'hilari', 'silli', 'vulgar', 'sampl', 'line', 'wanna', 'shake', 'nake', 'eat', 'aliv', 'better', 'still', 'flick', 'certainli', 'deliv', 'plenti', 'tasti', 'femal', 'nuditi', 'gorgeous', 'statuesqu', 'anderson', 'look', 'smoke', 'hot', 'buff', 'sizzl', 'semi', 'pornograph', 'soft', 'core', 'sex', 'scene', 'johnni', 'zandale', 'dirti', 'deed', 'church', 'confession', 'booth', 'rate', 'definit', 'steami', 'highlight', 'tart', 'n', 'tangi', 'new', 'orlean', 'set', 'add', 'extra', 'spice', 'alreadi', 'steami', 'proceed', 'long', 'scruffi', 'black', 'hair', 'greasi', 'mustach', 'foul', 'mouth', 'coars', 'manner', 'cage', 'johnni', 'absolut', 'hoot', 'singl', 'grossli', 'unapp', 'romant', 'lead', 'ever', 'ooz', 'way', 'onto', 'celluloid', 'cast', 'deserv', 'prop', 'act', 'admir', 'sincer', 'anderson', 'cage', 'reinhold', 'respect', 'work', 'part', 'fine', 'support', 'joe', 'pantoliano', 'zandale', 'merri', 'flamboy', 'homosexu', 'friend', 'gerri', 'viveca', 'lindfor', 'theirri', 'wise', 'percept', 'mother', 'tatta', 'aaron', 'nevil', 'friendli', 'bartend', 'jack', 'steve', 'buscemi', 'funni', 'blith', 'shameless', 'thief', 'walt', 'lloyd', 'sharp', 'gleam', 'cinematographi', 'give', 'pictur', 'attract', 'glossi', 'look', 'flavorsom', 'harmon', 'score', 'pray', 'rain', 'likewis', 'hit', 'spot', 'delight', 'campi', 'seami', 'riot']\n",
      "\n",
      "--- Label ---\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = list(map(review_to_words, data_train))\n",
    "        words_test = list(map(review_to_words, data_test))\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test\n",
    "\n",
    "# Preprocess data\n",
    "words_train, words_test, labels_train, labels_test = preprocess_data(\n",
    "        data_train, data_test, labels_train, labels_test)\n",
    "\n",
    "# Take a look at a sample\n",
    "print(\"\\n--- Raw review ---\")\n",
    "print(data_train[1])\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[1])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extracting Bag-of-Words features <a class=\"anchor\" id=\"step3\"></a>\n",
    "\n",
    "Now that each document has been preprocessed, we can transform each into a Bag-of-Words feature representation. Note that we need to create this transformation based on the training data alone, as we are not allowed to peek at the testing data at all.\n",
    "\n",
    "The dictionary or _vocabulary_ $V$ (set of words shared by documents in the training set) used here will be the one on which we train our supervised learning algorithm. Any future test data must be transformed in the same way for us to be able to apply the learned model for prediction. Hence, it is important to store the transformation / vocabulary as well.\n",
    "\n",
    "> **Note**: The set of words in the training set may not be exactly the same as the test set. What do we do if we encounter a word during testing that we haven't seen before? Unfortunately, we'll have to ignore it, or replace it with a special `<UNK>` token.\n",
    "\n",
    "### Compute Bag-of-Words features\n",
    "\n",
    "We are going to implement the `extract_BoW_features()` function, apply it to both training and test datasets, and store the results in `features_train` and `features_test` Numpy arrays, respectively. We will choose a reasonable vocabulary size, say $|V| = 5000$, and keep only the top $|V|$ occuring words and discard the rest. This number will also serve as the number of columns in the BoW matrices.\n",
    "\n",
    "We will take advantage of `CountVectorizer` from scikit-learn, and pickle the Bag-of-Words transformation so that we can use it in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features from cache file: bow_features.pkl\n",
      "Vocabulary: 5000 words\n",
      "Sample words: ['sleazi', 'clash', 'knew', 'hand', 'reson', 'conflict', 'underli', 'forgiven']\n",
      "\n",
      "--- Preprocessed words ---\n",
      "['nine', 'ten', 'might', 'seem', 'like', 'high', 'mark', 'give', 'straight', 'video', 'sci', 'fi', 'movi', 'vilifi', 'us', 'box', 'offic', 'roundli', 'critic', 'poorest', 'movi', 'kurt', 'russel', 'career', 'reason', 'firstli', 'read', 'neg', 'review', 'film', 'usual', 'start', 'wooden', 'natur', 'russel', 'interpret', 'todd', 'eponym', 'soldier', 'go', 'start', 'surpris', 'statement', 'possibl', 'finest', 'piec', 'act', 'seen', 'russel', 'pull', 'todd', 'emot', 'crippl', 'suffer', 'intens', 'ptsd', 'movi', 'written', 'phenomenon', 'wide', 'recogn', 'portray', 'spot', 'todd', 'withdrawn', 'uncommun', 'loner', 'suffer', 'irrat', 'anxieti', 'key', 'fever', 'pitch', 'train', 'teach', 'analyz', 'everi', 'movement', 'interact', 'anoth', 'human', 'sign', 'betray', 'danger', 'hyper', 'focu', 'bring', 'inabl', 'comprehend', 'bigger', 'continuum', 'task', 'given', 'sit', 'within', 'scene', 'cut', 'slice', 'carrot', 'continu', 'work', 'unfaz', 'clean', 'cut', 'blood', 'mani', 'interpret', 'sign', 'physic', 'tough', 'focu', 'job', 'hand', 'also', 'sign', 'simpli', 'perform', 'request', 'task', 'rote', 'comprehend', 'relationship', 'veget', 'prepar', 'food', 'eaten', 'later', 'todd', 'dialog', 'spartan', 'say', 'least', 'two', 'big', 'talk', 'scene', 'get', 'central', 'plot', 'movi', 'underlin', 'bleak', 'natur', 'exist', 'fear', 'disciplin', 'told', 'alway', 'fear', 'keep', 'pump', 'hyper', 'alert', 'state', 'smallest', 'detail', 'pass', 'keep', 'readi', 'react', 'knife', 'edg', 'disciplin', 'hold', 'check', 'fear', 'overcom', 'perform', 'tactic', 'infer', 'time', 'think', 'cannot', 'afford', 'feel', 'mani', 'viewer', 'differ', 'interpret', 'reaction', 'hug', 'nielsen', 'sandra', 'believ', 'interpret', 'perspect', 'human', 'experi', 'embrac', 'combat', 'trembl', 'repres', 'suppress', 'fight', 'flight', 'instinct', 'react', 'fear', 'grappl', 'movement', 'vision', 'restrict', 'fear', 'disciplin', 'inde', 'subtext', 'abandon', 'twice', 'fact', 'repres', 'way', 'societi', 'tend', 'toss', 'infantrymen', 'onto', 'rubbish', 'heap', 'societi', 'serv', 'term', 'unemploy', 'ex', 'militari', 'world', 'wors', 'russel', 'quickli', 'pick', 'mantl', 'mace', 'respons', 'wife', 'child', 'desper', 'need', 'mission', 'even', 'one', 'high', 'likelihood', 'death', 'militari', 'subtext', 'conflict', 'busey', 'church', 'hotshot', 'hq', 'mekum', 'mekum', 'new', 'men', 'faster', 'stronger', 'accur', 'aggress', 'one', 'could', 'pound', 'todd', 'ground', 'tool', 'use', 'incentiv', 'todd', 'given', 'freedom', 'exercis', 'initi', 'act', 'without', 'numb', 'effect', 'perceiv', 'superior', 'util', 'ambush', 'tactic', 'sneaki', 'trick', 'cut', 'swath', 'newer', 'unit', 'sent', 'without', 'support', 'cover', 'reconnaiss', 'remind', 'militari', 'power', 'cannot', 'make', 'failur', 'leadership', 'mani', 'subtl', 'theme', 'film', 'shot', 'script', 'minim', 'leav', 'plenti', 'white', 'space', 'interpret', 'take', 'root', 'watch', 'soldier', 'open', 'mind', 'see', 'teach']\n",
      "\n",
      "--- Bag-of-Words features ---\n",
      "[1 0 0 ... 0 0 0]\n",
      "\n",
      "--- Label ---\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing Numpy arrays\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # Training documents have already been preprocessed and tokenized into words, so\n",
    "        # pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "        # Convert the features using .toarray() for a compact representation\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary\n",
    "\n",
    "\n",
    "# Extract Bag of Words features for both training and test datasets\n",
    "features_train, features_test, vocabulary = extract_BoW_features(words_train, words_test)\n",
    "\n",
    "# Inspect the vocabulary that was computed\n",
    "print(\"Vocabulary: {} words\".format(len(vocabulary)))\n",
    "\n",
    "import random\n",
    "print(\"Sample words: {}\".format(random.sample(list(vocabulary.keys()), 8)))\n",
    "\n",
    "# Sample\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[5])\n",
    "print(\"\\n--- Bag-of-Words features ---\")\n",
    "print(features_train[5])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize the Bag-of-Words feature vector for one of our training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e81f1a9d0704>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot the BoW feature vector for a training document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Word'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Count'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the BoW feature vector for a training document\n",
    "plt.plot(features_train[5,:])\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: Reflecting on Bag-of-Words feature representation\n",
    "\n",
    "The average sparsity level of BoW vectors in our training set (i.e. the percentage of entries in a BoW feature vector that are zero) is ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sparsity level of BoW vectors in training set is 98%\n"
     ]
    }
   ],
   "source": [
    "print (\"The average sparsity level of BoW vectors in training set is {0:.0f}%\".format((np.count_nonzero(features_train==0)*100)/features_train.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's law\n",
    "\n",
    "[Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law), named after the famous American linguist George Zipf, is an empirical law stating that given a large collection of documents, the frequency of any word is inversely proportional to its rank in the frequency table. So the most frequent word will occur about twice as often as the second most frequent word, three times as often as the third most frequent word, and so on. The figure below shows the number of appearances of each word in the training set against its rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-687866d8acab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_word_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Find number of occurrences for each word in the training set\n",
    "word_freq = features_train.sum(axis=0)\n",
    "\n",
    "# Sort it in descending order\n",
    "sorted_word_freq = np.sort(word_freq)[::-1]\n",
    "\n",
    "# Plot \n",
    "plt.plot(sorted_word_freq)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: Zipf's law\n",
    "\n",
    "The following cell shows the total number of occurrences of the most frequent word and that of the second most frequent word. The numbers do not follow Zipf's law. It must be because of the removal of the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurrences of the most frequent word: 51696\n",
      "Number of occurrences of the 2nd most frequent word: 48191\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of occurrences of the most frequent word: {}\".format(sorted_word_freq[0]))\n",
    "print (\"Number of occurrences of the 2nd most frequent word: {}\".format(sorted_word_freq[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize feature vectors\n",
    "\n",
    "Bag-of-Words features are intuitive to understand as they are simply word counts. But counts can vary a lot, and potentially throw off learning algorithms later in the pipeline. So, before we proceed further, let's normalize the BoW feature vectors to have unit length.\n",
    "\n",
    "This makes sure that each document's representation retains the unique mixture of feature components, but prevents documents with large word counts from dominating those with fewer words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pr\n",
    "\n",
    "# Normalize BoW features in training and test set\n",
    "features_train = pr.normalize(features_train)\n",
    "features_test = pr.normalize(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classification using BoW features <a class=\"anchor\" id=\"step4\"></a>\n",
    "\n",
    "Now that the data has all been properly transformed, we can feed it into a classifier. To get a baseline model, we train a Naive Bayes classifier from scikit-learn (specifically, [`GaussianNB`](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)), and evaluate its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GaussianNB] Accuracy: train = 0.81892, test = 0.72452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train a Guassian Naive Bayes classifier\n",
    "clf1 = GaussianNB()\n",
    "clf1.fit(features_train, labels_train)\n",
    "\n",
    "# Calculate the mean accuracy score on training and test sets\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf1.__class__.__name__,\n",
    "        clf1.score(features_train, labels_train),\n",
    "        clf1.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based algorithms often work quite well on Bag-of-Words as their highly discontinuous and sparse nature is nicely matched by the structure of trees. I will try to improve on the Naive Bayes classifier's performance by using scikit-learn's Gradient-Boosted Decision Tree classifer.\n",
    "\n",
    "### Gradient-Boosted Decision Tree classifier\n",
    "\n",
    "Use [`GradientBoostingClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) from scikit-learn to classify the BoW data. This model has a number of parameters. I will use default values for some of them, except one: `n_estimators` which I will tune using grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END ...................................n_estimators=160; total time= 9.0min\n",
      "[CV] END ...................................n_estimators=160; total time=10.2min\n",
      "[CV] END ...................................n_estimators=160; total time=10.8min\n",
      "[CV] END ...................................n_estimators=160; total time=10.1min\n",
      "[CV] END ...................................n_estimators=160; total time=10.2min\n",
      "Best n_estimators:  {'n_estimators': 160}\n",
      "[GradientBoostingClassifier] Accuracy: train = 0.86256, test = 0.84244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The best n_estimators is 160\n",
    "# Here is the range on which I performed GridSearchCV: params = {\"n_estimators\": range(10, 200, 10)}\n",
    "# Use only 160 to avoid rerunning gridsearch on the whole range(10,200,10)\n",
    "params = {\"n_estimators\": [160]}\n",
    "\n",
    "def classify_gboost(X_train, X_test, y_train, y_test):        \n",
    "    # Initialize classifier\n",
    "    gbc = GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    \n",
    "    # Perform hyperparameter tuning\n",
    "    grid_search = GridSearchCV(gbc, params, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "    print (\"Best n_estimators: \", grid_search.best_params_)\n",
    "    \n",
    "    # Print final training & test accuracy\n",
    "    print (\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "    clf.__class__.__name__,\n",
    "    clf.score(X_train, y_train),\n",
    "    clf.score(X_test, labels_test)))\n",
    "    # Return best classifier model\n",
    "    return clf\n",
    "\n",
    "\n",
    "clf2 = classify_gboost(features_train, features_test, labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adverserial testing\n",
    "\n",
    "I've written a short movie review to trick the GBDT model. That is, a movie review with a clear positive or negative sentiment that the model will classify incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos']\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Write a sample review and set its true sentiment\n",
    "my_review = \"The actor's initial superb performance became average later on\"\n",
    "true_sentiment = 'neg'  # sentiment must be 'pos' or 'neg'\n",
    "\n",
    "# Apply the same preprocessing and vectorizing steps done for the training data\n",
    "my_words = review_to_words(my_review)\n",
    "my_vectorizer = CountVectorizer(vocabulary=vocabulary, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "my_features = my_vectorizer.transform([my_words]).toarray()\n",
    "my_features = pr.normalize(my_features)\n",
    "\n",
    "# Then call the classifier to label it\n",
    "print (clf2.predict(my_features))\n",
    "print (clf2.score(my_features, [true_sentiment]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Logistic Regression Classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression] Accuracy: train = 0.89696, test = 0.8744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train an lR classifier\n",
    "clf2 = LogisticRegression()\n",
    "clf2.fit(features_train, labels_train)\n",
    "\n",
    "# Calculate the mean accuracy score on training and test sets\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf2.__class__.__name__,\n",
    "        clf2.score(features_train, labels_train),\n",
    "        clf2.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decision Tree Classifier(AdaBoost and Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=20\n",
    ")\n",
    "clf3.fit(features_train, labels_train)\n",
    "\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf3.__class__.__name__,\n",
    "        clf3.score(features_train, labels_train),\n",
    "        clf3.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = RandomForestClassifier(max_depth=35, random_state=0)\n",
    "clf4.fit(features_train,labels_train)\n",
    "\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf4.__class__.__name__,\n",
    "        clf4.score(features_train, labels_train),\n",
    "        clf4.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 73508)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 74535)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: SVM ( Support Vector Machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LinearSVC] Accuracy: train = 0.93148, test = 0.8722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "svc=LinearSVC(random_state= 0 ,max_iter=15000)\n",
    "svc.fit(features_train,labels_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        svc.__class__.__name__,\n",
    "        svc.score(features_train, labels_train),\n",
    "        svc.score(features_test, labels_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Switching gears - RNNs <a class=\"anchor\" id=\"step5\"></a>\n",
    "\n",
    "We just saw how the task of sentiment analysis can be solved via a traditional machine learning approach: BoW + a nonlinear classifier. We now switch gears and use Recurrent Neural Networks, and in particular LSTMs, to perform sentiment analysis in Keras. Conveniently, Keras has a built-in [IMDb movie reviews dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) that we can use, with the same vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the built-in imdb dataset in Keras\n",
    "from keras.datasets import imdb  \n",
    "\n",
    "# Set the vocabulary size\n",
    "vocabulary_size = 5000\n",
    "\n",
    "# Load in training and test data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "print(\"Loaded dataset with {} training samples, {} test samples\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample review and its label\n",
    "print(\"--- Review ---\")\n",
    "print(X_train[7])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the label is an integer (0 for negative, 1 for positive), and the review itself is stored as a sequence of integers. These are word IDs that have been preassigned to individual words. To map them back to the original words, we can use the dictionary returned by `imdb.get_word_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map word IDs back to words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print(\"--- Review (with words) ---\")\n",
    "print([id2word.get(i, \" \") for i in X_train[7]])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our Bag-of-Words approach, where we simply summarized the counts of each word in a document, this representation essentially retains the entire sequence of words (minus punctuation, stopwords, etc.). This is critical for RNNs to function. But it also means that now the features can be of different lengths!\n",
    "\n",
    "#### Comments: Variable length reviews\n",
    "\n",
    "Below are the maximum review length (in terms of number of words) and the minimum in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The review that has the max length\n",
    "longest_review = max(X_train, key=len)\n",
    "\n",
    "# The length of the longest review\n",
    "print (len(longest_review))\n",
    "\n",
    "# The length of the shortest review\n",
    "print (len(min(X_train, key=len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences\n",
    "\n",
    "In order to feed this data into an RNN, all input documents must have the same length. Let's limit the maximum review length to `max_words` by truncating longer reviews and padding shorter reviews with a null value (0). We can accomplish this easily using the [`pad_sequences()`](https://keras.io/preprocessing/sequence/#pad_sequences) function in Keras. For now, set `max_words` to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Set the maximum number of words per document (for both training and testing)\n",
    "max_words = 500\n",
    "\n",
    "# Pad sequences in X_train and X_test\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design an RNN model for sentiment analysis\n",
    "\n",
    "I am going to build an RNN using LSTM. The input is a sequence of words (technically, integer word IDs) of maximum length = `max_words`, and the output is a binary sentiment label (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "#from keras.optimizers import Adam\n",
    "from keras.optimizers import adam_v2\n",
    "\n",
    "learning_rate = 0.001\n",
    "embed_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embed_dim, input_length=max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the RNN model\n",
    "\n",
    "Now we are ready to train the model. In Keras world, first we need to _compile_ the model by specifying the loss function and optimizer we want to use while training, as well as any evaluation metrics we'd like to measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model, specifying a loss function, optimizer, and metrics\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once compiled, we can kick off the training process. There are two important training parameters that we have to specify - **batch size** and **number of training epochs**, which together with the model architecture determine the total training time. I will use the typical value of 32 for `batch_size`. I will select a large number of epochs, 50, but will also use `EarlyStopping` so that the training will stop if validation loss doesn't fall for a number consecutive epochs. This technique usually saves time from training the model for too long without any improvement.\n",
    "\n",
    "I will also split off a small portion of the training set to be used for validation during training. This will help monitor the training process and identify potential overfitting. I will use `validation_split` - a fraction of the training data for Keras to set aside for validation purpose. Validation metrics are evaluated once at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Specify training parameters: batch size and number of epochs\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "# Save the model, so that we can quickly load it in future (and perhaps resume training)\n",
    "model_file = \"rnn_model.h5\"  # HDF5 file\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join(cache_dir, model_file), verbose=1, \n",
    "                               save_best_only=True)\n",
    "# Stop training if the validation loss doesn't fall for 3 consecutive epochs\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "# Train the model, reserve some training data for validation\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_split=0.2, callbacks=[checkpointer, earlystopping], \n",
    "          verbose=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later we can load it using keras.models.load_model()\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(cache_dir, model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained the model, it's time to see how well it performs on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
    "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy that I passed in metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation <a class=\"anchor\" id=\"step6\"></a>\n",
    "\n",
    "### Comparing RNNs and Traditional Methods\n",
    "\n",
    "The BoW + Gradient-Boosted Decision Trees (GBDT) model does a better job than the BoW + GaussianNB model. The RNN model outperforms both models. LSTM is able to remember over long sequences of inputs while the BoW models disregard word order. As seen in the adverserial testing part of BoW + GBDT, this limitation must have caused the model to classify the tricky review incorrectly.\n",
    "\n",
    "### Steps taken to improve the accuracy of the RNN model\n",
    "\n",
    "Below is the initial attempt that yielded an accuracy of 0.86792:\n",
    "\n",
    "```\n",
    "learning_rate = 0.01\n",
    "embed_dim = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embed_dim, input_length=max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "`Dropout` helped reduce overfitting. \n",
    "\n",
    "We then did the following experiments:\n",
    "\n",
    "* Reduced the `learning_rate`. Higher learning rate, 0.01, must have caused the optimizer to overshoot the minimum. Lower rate made training more reliable, but also slower.\n",
    "* Tried various values for `embed_dim` (64, 100, 128, and 300). 300 yielded the best validation and test accuracies, although at the cost of training time.\n",
    "* Tried a) 1 LSTM layer (128 units) and b) 3 LSTM layers (32 units each). a) showed a better performance.\n",
    "* Tried GRU instead of LSTM and the performance was similar.\n",
    "\n",
    "After the experiments, the final model (trained above) achieved an accuracy of 0.88288.\n",
    "\n",
    "```\n",
    "learning_rate = 0.001\n",
    "embed_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embed_dim, input_length=max_words))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
